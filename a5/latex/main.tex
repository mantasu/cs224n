\documentclass[answers]{exam}
\newif\ifanswers
\answerstrue% comment out to hide answers

\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{soul}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{hyperref}
\usepackage{datetime}
\usepackage{minted}
\usepackage{xspace}
\settimeformat{ampmtime}
\usepackage{listings}
\usepackage{inconsolata}

\usepackage{array}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{tikz}
\usetikzlibrary{positioning,patterns,fit,calc}
% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
%\pagestyle{fancy}
%\rhead{\hmwkAuthorName} % Top left header
%\lhead{\hmwkClass: \hmwkTitle} % Top center head
%\lfoot{\lastxmark} % Bottom left footer
%\cfoot{} % Bottom center footer
%\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
%\renewcommand\headrulewidth{0.4pt} % Size of the header rule
%\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\newcommand{\mingpt}{\texttt{minGPT}\xspace}
\newcommand{\ourmodel}{\texttt{minLinT5}\xspace}

\pagestyle{headandfoot}
\runningheadrule{}
\firstpageheader{CS 224N}{Assignment 5}{Self-attention, Transformers, Pretraining}
\runningheader{CS 224N} {Assignment 5} {Page \thepage\ of \numpages}
\firstpagefooter{}{}{} \runningfooter{}{}{}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Python} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Python, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\footnotesize\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
 

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment 5: Self-Attention, Transformers, and Pretraining} % Assignment title
\newcommand{\hmwkClass}{CS\ 224N} % Course/class
\newcommand{\ifans}[1]{\ifanswers \color{red}  \textbf{Solution: } #1 \color{black} 
 \fi}
% \vspace{5mm} \fi}

\input std-macros
\input macros

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\qformat{\Large\bfseries\thequestion{}. \thequestiontitle{} (\thepoints{})\hfill}

\title{
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}
}
\author{}
%\date{\textit{\small Updated \today\ at \currenttime}} % Insert date here if you want it to appear below your name
\date{}

\setcounter{section}{0} % one-indexing
\begin{document}

\vspace{-1in}

\maketitle

\vspace{-0.5in}


\begin{framed}
\noindent
 \textbf{Note.} Here are some things to keep in mind as you plan your time for this assignment.
\begin{itemize}
   \item There are math questions again!
   \item The total amount of PyTorch code to write, and code complexity, of this assignment is lower than Assignment 4. 
         However, you're also given less guidance or scaffolding in how to write the code.
         \item  This assignment involves a pretraining step that takes approximately 2 hours to perform on Azure, and you'll have to do it twice.
\end{itemize}
\end{framed}
This assignment is an investigation into Transformer self-attention building blocks, and the effects of pretraining.
It covers mathematical properties of Transformers and self-attention through written questions.
Further, you'll get experience with practical system-building through repurposing an existing codebase.
The assignment is split into a written (mathematical) part and a coding part, with its own written questions.
Here's a quick summary:
\begin{enumerate}
\item \textbf{Mathematical exploration:}  What kinds of operations can self-attention easily implement? Why should we use fancier things like multi-headed self-attention?
This section will use some mathematical investigations to illuminate a few of the motivations of self-attention and Transformer networks.
{\color{red} \textbf{Note:} for all questions, you should justify your answer with mathematical reasoning when required.}
\item \textbf{Extending a research codebase:}
In this portion of the assignment, you'll get some experience and intuition for a cutting-edge research topic in NLP: teaching NLP models facts about the world through pretraining, and accessing that knowledge through finetuning.
You'll train a Transformer model to attempt to answer simple questions of the form ``Where was person [x] born?'' -- without providing any input text from which to draw the answer.
You'll find that models are able to learn some facts about where people were born through pretraining, and access that information during fine-tuning to answer the questions.

Then, you'll take a harder look at the system you built, and reason about the implications and concerns about relying on such implicit pretrained knowledge.

\end{enumerate}

This assignment was originally created by John Hewitt, CS 224N Head TA in Winter 2021.



\newpage

\begin{questions}
   \input q_math
   \input q_code
\end{questions}

\Large{\textbf{Submission Instructions}}

\normalsize
You will submit this assignment on GradeScope as two submissions -- one for \textbf{Assignment 5 [coding]} and another for \textbf{Assignment 5 [written]}:
\begin{enumerate}
    \item Verify that the following files exist at these specified paths within your assignment directory:
        \begin{itemize}
            \item The no-pretraining model and predictions: \texttt{vanilla.model.params}, \texttt{vanilla.nopretrain.dev.predictions},\\\texttt{vanilla.nopretrain.test.predictions}
            \item The pretrain-finetune model and predictions: \texttt{vanilla.finetune.params}, \texttt{vanilla.pretrain.dev.predictions}, \\ \texttt{vanilla.pretrain.test.predictions}
            \item The synthesizer model and predictions: \texttt{synthesizer.finetune.params}, \texttt{synthesizer.pretrain.dev.predictions}, \\ \texttt{synthesizer.pretrain.test.predictions}
        \end{itemize}

    \item Run the \texttt{collect\_submission.sh} script to produce your \texttt{assignment5.zip} file.
    \item Upload your \texttt{assignment5.zip} file to GradeScope to \textbf{Assignment 5 [coding]}.
    \item Check that the public autograder tests passed correctly.
    \item Upload your written solutions, for questions 1, parts of 2, and 3, to GradeScope to \textbf{Assignment 5 [written]}. Tag it properly!
\end{enumerate}

\bibliographystyle{acm}
\bibliography{main}

\end{document}
